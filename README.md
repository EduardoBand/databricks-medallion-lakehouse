# Data Lakehouse: Sales Pipeline with Medallion Architecture 

![Databricks](https://img.shields.io/badge/Databricks-FF6F00?style=flat&logo=databricks) 
![PySpark](https://img.shields.io/badge/PySpark-EE4C2C?style=flat&logo=apache-spark) 
![Delta Lake](https://img.shields.io/badge/Delta%20Lake-3F51B5?style=flat&logo=delta-lake) 
![Python](https://img.shields.io/badge/Python-3776AB?style=flat&logo=python)

Este reposit√≥rio apresenta um **pipeline completo de Data Engineering** no **Databricks**, usando **PySpark** e **Delta Lake**.  
O projeto implementa a arquitetura **Medallion (Bronze, Silver e Gold)** para processar, transformar e otimizar dados de vendas para an√°lise e BI.

---

## üéØ Objetivos do Projeto
- Construir um pipeline **end-to-end** de dados de vendas.  
- Aplicar o conceito de **Medallion Architecture**.  
- Garantir **qualidade, rastreabilidade e efici√™ncia** no processamento de dados.  
- Entregar tabelas prontas para an√°lises e dashboards.

---

## üèóÔ∏è Arquitetura do Pipeline

**Fluxo de dados:**

Landing Zone ‚Üí Bronze Layer ‚Üí Silver Layer ‚Üí Gold Layer ‚Üí BI / Analytics

- **Landing Zone**: Ingest√£o de dados brutos de fontes externas.  
- **Bronze Layer**: Armazena dados brutos em **Delta Lake** para auditoria e hist√≥rico.  
- **Silver Layer**: Limpeza, tratamento de nulos, valida√ß√£o de schema e enriquecimento.  
- **Gold Layer**: Tabelas modeladas em **Star Schema** (Fatos e Dimens√µes) otimizadas para BI.

---

## üìÅ Estrutura do Reposit√≥rio

| Notebook | Descri√ß√£o |
|----------|-----------|
| `000 Setup DBFS` | Configura√ß√£o inicial e montagem de diret√≥rios. |
| `001 Import Data` | Ingest√£o automatizada para a Landing Zone. |
| `002 Load Bronze` | Carga inicial de dados brutos em Delta. |
| `003 Silver Transformation` | Limpeza, cast de tipos e transforma√ß√µes intermedi√°rias. |
| `004 Load Gold Delta` | Modelagem dimensional com **Surrogate Keys**. |
| `005 Incremental Gold Load` | Processamento incremental para efici√™ncia. |
| `006 Optimized Queries` | Exemplos de queries SQL/Spark otimizadas. |
| `007 Delta Table Creation` | Registro das tabelas no **Hive Metastore / Unity Catalog**. |
| `008 Delta Maintenance` | Governan√ßa: `VACUUM`, `OPTIMIZE`, **Z-Ordering**. |

---

## üõ†Ô∏è Tecnologias & Recursos

- **PySpark**: Processamento distribu√≠do.  
- **Delta Lake**: ACID, Time Travel e versionamento.  
- **Star Schema**: Estrutura otimizada para an√°lise.  
- **Performance**: Z-Ordering, Optimize, Vacuum.  
- **Mem√≥ria**: `gc.collect()`, `unpersist()`, configura√ß√£o Spark.

---

## üöÄ Como Executar

1. Crie um workspace no **[Databricks](https://databricks.com/)**.  
2. Importe os notebooks da pasta `/notebooks`.  
3. Execute na **ordem num√©rica**.  
4. Configure um cluster Spark compat√≠vel com **Delta Lake** OU utilize o servi√ßo Serveless da Free Edition.
5. *Caso voc√™ tenha a vers√£o standard (ou +), voc√™ poder√° utilizar esses notebooks para gerar um Workflow do processo.*
6. Totalmente usavel em PowerBI.
---

## üîó Links √öteis
- [Documenta√ß√£o PySpark](https://spark.apache.org/docs/latest/api/python/)  
- [Delta Lake Docs](https://delta.io/)  
- [Databricks Free Edition](https://community.cloud.databricks.com/)  

---

‚ú® Este projeto segue as melhores pr√°ticas de **Data Engineering**, fornecendo um pipeline **escal√°vel, eficiente e pronto para an√°lises de vendas**.
