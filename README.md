# Data Lakehouse: Sales Pipeline with Medallion Architecture 

![Databricks](https://img.shields.io/badge/Databricks-FF6F00?style=flat&logo=databricks) 
![PySpark](https://img.shields.io/badge/PySpark-EE4C2C?style=flat&logo=apache-spark) 
![Delta Lake](https://img.shields.io/badge/Delta%20Lake-3F51B5?style=flat&logo=delta-lake) 
![Python](https://img.shields.io/badge/Python-3776AB?style=flat&logo=python)

Este repositÃ³rio apresenta um **pipeline completo de Data Engineering** no **Databricks**, usando **PySpark** e **Delta Lake**.  
O projeto implementa a arquitetura **Medallion (Bronze, Silver e Gold)** para processar, transformar e otimizar dados de vendas para anÃ¡lise e BI.

---

## ğŸ¯ Objetivos do Projeto
- Construir um pipeline **end-to-end** de dados de vendas.  
- Aplicar o conceito de **Medallion Architecture**.  
- Garantir **qualidade, rastreabilidade e eficiÃªncia** no processamento de dados.  
- Entregar tabelas prontas para anÃ¡lises e dashboards.

---

## ğŸ—ï¸ Arquitetura do Pipeline

**Fluxo de dados:**

Landing Zone â†’ Bronze Layer â†’ Silver Layer â†’ Gold Layer â†’ BI / Analytics

- **Landing Zone**: IngestÃ£o de dados brutos de fontes externas.  
- **Bronze Layer**: Armazena dados brutos em **Delta Lake** para auditoria e histÃ³rico.  
- **Silver Layer**: Limpeza, tratamento de nulos, validaÃ§Ã£o de schema e enriquecimento.  
- **Gold Layer**: Tabelas modeladas em **Star Schema** (Fatos e DimensÃµes) otimizadas para BI.

---

## ğŸ“ Estrutura do RepositÃ³rio

| Notebook | DescriÃ§Ã£o |
|----------|-----------|
| `000 Setup DBFS` | ConfiguraÃ§Ã£o inicial e montagem de diretÃ³rios. |
| `001 Import Data` | IngestÃ£o automatizada para a Landing Zone. |
| `002 Load Bronze` | Carga inicial de dados brutos em Delta. |
| `003 Silver Transformation` | Limpeza, cast de tipos e transformaÃ§Ãµes intermediÃ¡rias. |
| `004 Load Gold Delta` | Modelagem dimensional com **Surrogate Keys**. |
| `005 Incremental Gold Load` | Processamento incremental para eficiÃªncia. |
| `006 Optimized Queries` | Exemplos de queries SQL/Spark otimizadas. |
| `007 Delta Table Creation` | Registro das tabelas no **Hive Metastore / Unity Catalog**. |
| `008 Delta Maintenance` | GovernanÃ§a: `VACUUM`, `OPTIMIZE`, **Z-Ordering**. |

---

## ğŸ› ï¸ Tecnologias & Recursos

- **PySpark**: Processamento distribuÃ­do.  
- **Delta Lake**: ACID, Time Travel e versionamento.  
- **Star Schema**: Estrutura otimizada para anÃ¡lise.  
- **Performance**: Z-Ordering, Optimize, Vacuum.  
- **MemÃ³ria**: `gc.collect()`, `unpersist()`, configuraÃ§Ã£o Spark.

---

## ğŸš€ Como Executar

1. Crie um workspace no **[Databricks](https://databricks.com/)**.  
2. Importe os notebooks da pasta `/notebooks`.  
3. Execute na **ordem numÃ©rica**.  
4. Configure um cluster Spark compatÃ­vel com **Delta Lake** OU utilize o serviÃ§o Serveless da Free Edition.
5. *Caso vocÃª tenha a versÃ£o standard (ou +), vocÃª poderÃ¡ utilizar esses notebooks para gerar um Workflow do processo.*  

---

## ğŸ”— Links Ãšteis
- [DocumentaÃ§Ã£o PySpark](https://spark.apache.org/docs/latest/api/python/)  
- [Delta Lake Docs](https://delta.io/)  
- [Databricks Community Edition](https://community.cloud.databricks.com/)  

---

âœ¨ Este projeto segue as melhores prÃ¡ticas de **Data Engineering**, fornecendo um pipeline **escalÃ¡vel, eficiente e pronto para anÃ¡lises de vendas**.
